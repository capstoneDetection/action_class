{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading model: [Errno 2] No such file or directory: 'Models/stgcn_fall_detection.pth'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1743945927.249937       1 gl_context.cc:344] GL version: 2.1 (2.1 Metal - 88.1), renderer: Apple M1\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "2025-04-06 22:25:27.535 python[21163:4433252] WARNING: AVCaptureDeviceTypeExternal is deprecated for Continuity Cameras. Please use AVCaptureDeviceTypeContinuityCamera and add NSCameraUseContinuityCameraDeviceType to your Info.plist.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m현재 셀 또는 이전 셀에서 코드를 실행하는 동안 Kernel이 충돌했습니다. \n",
      "\u001b[1;31m셀의 코드를 검토하여 가능한 오류 원인을 식별하세요. \n",
      "\u001b[1;31m자세한 내용을 보려면 <a href='https://aka.ms/vscodeJupyterKernelCrash'>여기</a>를 클릭하세요. \n",
      "\u001b[1;31m자세한 내용은 Jupyter <a href='command:jupyter.viewOutput'>로그</a>를 참조하세요."
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Set device (use GPU if available, otherwise CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define the ST-GCN model class (unchanged)\n",
    "class STGCN(nn.Module):\n",
    "    def __init__(self, in_channels=3, num_joints=18, num_classes=2):\n",
    "        super(STGCN, self).__init__()\n",
    "        self.num_joints = num_joints\n",
    "        self.conv1 = nn.Conv2d(in_channels, 64, kernel_size=(1, 1))\n",
    "        self.conv2 = nn.Conv2d(64, 64, kernel_size=(3, 1), padding=(1, 0))\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=(1, 1))\n",
    "        self.conv4 = nn.Conv2d(128, 128, kernel_size=(3, 1), padding=(1, 0))\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.bn4 = nn.BatchNorm2d(128)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.pool = nn.AdaptiveAvgPool2d((1, num_joints))\n",
    "        self.fc = nn.Linear(128 * num_joints, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.relu(self.bn3(self.conv3(x)))\n",
    "        x = self.relu(self.bn4(self.conv4(x)))\n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# Load the pre-trained model (unchanged)\n",
    "model_path = \"Models/stgcn_fall_detection.pth\"\n",
    "try:\n",
    "    model = STGCN(in_channels=3, num_joints=18, num_classes=2).to(device)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.eval()\n",
    "    print(f\"Model loaded successfully from {model_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model: {e}\")\n",
    "    exit()\n",
    "\n",
    "# Initialize MediaPipe Pose (unchanged)\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose(static_image_mode=False, model_complexity=1, \n",
    "                    enable_segmentation=False, min_detection_confidence=0.5)\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Define the 18 landmarks in the same order as used during training (unchanged)\n",
    "landmark_order = [\n",
    "    \"Nose\", \"Neck\", \"Right Shoulder\", \"Right Elbow\", \"Right Wrist\",\n",
    "    \"Left Shoulder\", \"Left Elbow\", \"Left Wrist\", \"Right Hip\", \"Right Knee\",\n",
    "    \"Right Ankle\", \"Left Hip\", \"Left Knee\", \"Left Ankle\", \"Right Eye\",\n",
    "    \"Left Eye\", \"Right Ear\", \"Left Ear\"\n",
    "]\n",
    "\n",
    "landmark_indices = {\n",
    "    \"Nose\": 0, \"Right Shoulder\": 12, \"Right Elbow\": 14, \"Right Wrist\": 16,\n",
    "    \"Left Shoulder\": 11, \"Left Elbow\": 13, \"Left Wrist\": 15, \"Right Hip\": 24,\n",
    "    \"Right Knee\": 26, \"Right Ankle\": 28, \"Left Hip\": 23, \"Left Knee\": 25,\n",
    "    \"Left Ankle\": 27, \"Right Eye\": 5, \"Left Eye\": 2, \"Right Ear\": 8, \"Left Ear\": 7\n",
    "}\n",
    "\n",
    "# Initialize buffer to store skeleton data (no fixed size limit)\n",
    "buffer = []\n",
    "window_size = 30  # Number of frames per window\n",
    "step_size = 5    # Step size for sliding window (adjustable)\n",
    "\n",
    "# Initial threshold for fall detection\n",
    "threshold = 0.8\n",
    "\n",
    "# Start capturing video from the default camera\n",
    "cap = cv2.VideoCapture(0)\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Unable to open camera.\")\n",
    "    exit()\n",
    "\n",
    "# Main loop for real-time processing\n",
    "while True:\n",
    "    success, image = cap.read()\n",
    "    if not success:\n",
    "        print(\"Error: Unable to read frame from camera.\")\n",
    "        break\n",
    "    \n",
    "    # Convert the frame to RGB for MediaPipe processing\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    results = pose.process(image_rgb)\n",
    "    \n",
    "    # Initialize skeleton frame with zeros (18 joints, 3 coordinates: x, y, z)\n",
    "    skeleton_frame = np.zeros((18, 3))\n",
    "    \n",
    "    if results.pose_landmarks:\n",
    "        landmarks = results.pose_landmarks.landmark\n",
    "        \n",
    "        # Compute reference point: average of left and right hip\n",
    "        left_hip = landmarks[23]\n",
    "        right_hip = landmarks[24]\n",
    "        ref_x = (left_hip.x + right_hip.x) / 2\n",
    "        ref_y = (left_hip.y + right_hip.y) / 2\n",
    "        ref_z = (left_hip.z + right_hip.z) / 2\n",
    "        \n",
    "        # Compute neck as the average of left and right shoulders\n",
    "        left_shoulder = landmarks[11]\n",
    "        right_shoulder = landmarks[12]\n",
    "        neck_x = (left_shoulder.x + right_shoulder.x) / 2\n",
    "        neck_y = (left_shoulder.y + right_shoulder.y) / 2\n",
    "        neck_z = (left_shoulder.z + right_shoulder.z) / 2\n",
    "        \n",
    "        # Extract relative coordinates for the 18 landmarks\n",
    "        for i, part in enumerate(landmark_order):\n",
    "            if part == \"Neck\":\n",
    "                x = neck_x - ref_x\n",
    "                y = neck_y - ref_y\n",
    "                z = neck_z - ref_z\n",
    "            else:\n",
    "                lm = landmarks[landmark_indices[part]]\n",
    "                x = lm.x - ref_x\n",
    "                y = lm.y - ref_y\n",
    "                z = lm.z - ref_z\n",
    "            skeleton_frame[i] = [x, y, z]\n",
    "        \n",
    "        # Draw the detected landmarks on the image for visualization\n",
    "        mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS)\n",
    "    \n",
    "    # Add the current skeleton frame to the buffer\n",
    "    buffer.append(skeleton_frame)\n",
    "    \n",
    "    # Process sliding windows if enough frames are available\n",
    "    if len(buffer) >= window_size:\n",
    "        # Calculate the number of windows that can be processed\n",
    "        num_windows = (len(buffer) - window_size) // step_size + 1\n",
    "        latest_fall_prob = 0\n",
    "        latest_prediction = 0\n",
    "        \n",
    "        # Process the most recent window\n",
    "        start_idx = max(0, len(buffer) - window_size)\n",
    "        window = buffer[start_idx:start_idx + window_size]\n",
    "        \n",
    "        # Convert window to tensor with shape (1, 3, window_size, 18)\n",
    "        skeleton_sequence = np.stack(window, axis=0)  # Shape: (window_size, 18, 3)\n",
    "        skeleton_sequence = torch.tensor(skeleton_sequence, dtype=torch.float32).permute(2, 0, 1).unsqueeze(0)  # Shape: (1, 3, window_size, 18)\n",
    "        \n",
    "        # Perform inference\n",
    "        with torch.no_grad():\n",
    "            output = model(skeleton_sequence.to(device))\n",
    "            probabilities = torch.softmax(output, dim=1)\n",
    "            fall_prob = probabilities[0, 1].item()\n",
    "            prediction = 1 if fall_prob >= threshold else 0\n",
    "        \n",
    "        # Display the prediction and probability on the frame\n",
    "        if prediction == 1:\n",
    "            text = f\"Fall Detected! ({fall_prob:.2f})\"\n",
    "            color = (0, 0, 255)  # Red\n",
    "        else:\n",
    "            text = f\"No Fall ({fall_prob:.2f})\"\n",
    "            color = (0, 255, 0)  # Green\n",
    "        cv2.putText(image, text, (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, color, 2)\n",
    "        \n",
    "        # Display the current threshold and window info\n",
    "        info_text = f\"Threshold: {threshold:.2f} (+/-), Window: {window_size}, Step: {step_size}\"\n",
    "        cv2.putText(image, info_text, (50, 100), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "    else:\n",
    "        # Display buffering message until enough frames are collected\n",
    "        cv2.putText(image, f\"Buffering... ({len(buffer)}/{window_size})\", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "    \n",
    "    # Show the processed frame\n",
    "    cv2.imshow(\"Live Fall Detection\", image)\n",
    "    \n",
    "    # Handle key presses to adjust threshold, window size, step size, or exit\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    if key == ord('q'):  # Exit on 'q'\n",
    "        break\n",
    "    elif key == ord('+') or key == ord('='):  # Increase threshold\n",
    "        threshold = min(1.0, threshold + 0.05)\n",
    "    elif key == ord('-'):  # Decrease threshold\n",
    "        threshold = max(0.0, threshold - 0.05)\n",
    "    elif key == ord('w'):  # Increase window size\n",
    "        window_size = min(60, window_size + 5)\n",
    "    elif key == ord('s'):  # Decrease window size\n",
    "        window_size = max(10, window_size - 5)\n",
    "    elif key == ord('d'):  # Increase step size\n",
    "        step_size = min(10, step_size + 1)\n",
    "    elif key == ord('a'):  # Decrease step size\n",
    "        step_size = max(1, step_size - 1)\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "pose.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pose",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
