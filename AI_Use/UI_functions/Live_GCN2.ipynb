{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41fabc87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully from enhanced_stgcn_fall_detection.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1744344024.787235 2633731 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 88.1), renderer: Apple M1\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "W0000 00:00:1744344024.882593 2633925 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1744344024.897301 2633931 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "2025-04-11 13:00:25.057 python[31089:2633731] WARNING: AVCaptureDeviceTypeExternal is deprecated for Continuity Cameras. Please use AVCaptureDeviceTypeContinuityCamera and add NSCameraUseContinuityCameraDeviceType to your Info.plist.\n",
      "W0000 00:00:1744344036.347069 2633928 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define the 18 landmarks in the same order as used during training\n",
    "landmark_order = [\n",
    "    \"Nose\", \"Neck\", \"Right Shoulder\", \"Right Elbow\", \"Right Wrist\",\n",
    "    \"Left Shoulder\", \"Left Elbow\", \"Left Wrist\", \"Right Hip\", \"Right Knee\",\n",
    "    \"Right Ankle\", \"Left Hip\", \"Left Knee\", \"Left Ankle\", \"Right Eye\",\n",
    "    \"Left Eye\", \"Right Ear\", \"Left Ear\"\n",
    "]\n",
    "\n",
    "# Spatial graph edges for 18 joints (added to match training script)\n",
    "edges = [\n",
    "    (0, 1),   # Nose to Neck\n",
    "    (1, 2),   # Neck to Right Shoulder\n",
    "    (2, 3),   # Right Shoulder to Right Elbow\n",
    "    (3, 4),   # Right Elbow to Right Wrist\n",
    "    (1, 5),   # Neck to Left Shoulder\n",
    "    (5, 6),   # Left Shoulder to Left Elbow\n",
    "    (6, 7),   # Left Elbow to Left Wrist\n",
    "    (1, 8),   # Neck to Right Hip\n",
    "    (8, 9),   # Right Hip to Right Knee\n",
    "    (9, 10),  # Right Knee to Right Ankle\n",
    "    (1, 11),  # Neck to Left Hip\n",
    "    (11, 12), # Left Hip to Left Knee\n",
    "    (12, 13), # Left Knee to Left Ankle\n",
    "    (0, 14),  # Nose to Right Eye\n",
    "    (0, 15),  # Nose to Left Eye\n",
    "    (14, 16), # Right Eye to Right Ear\n",
    "    (15, 17)  # Left Eye to Left Ear\n",
    "]\n",
    "\n",
    "# Function to create adjacency matrix\n",
    "def create_adjacency_matrix(num_joints, edges):\n",
    "    A = np.zeros((num_joints, num_joints))\n",
    "    for edge in edges:\n",
    "        i, j = edge\n",
    "        A[i, j] = 1\n",
    "        A[j, i] = 1\n",
    "    A += np.eye(num_joints)\n",
    "    D = np.diag(np.sum(A, axis=1) ** -0.5)\n",
    "    A = D @ A @ D\n",
    "    return torch.tensor(A, dtype=torch.float32)\n",
    "\n",
    "# Graph Convolution Layer\n",
    "class GraphConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, A):\n",
    "        super(GraphConv, self).__init__()\n",
    "        self.A = A.to(device)\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=(1, 1))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch, channels, time, vertices = x.size()\n",
    "        x = x.permute(0, 2, 3, 1)\n",
    "        x = torch.matmul(self.A, x)\n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "# ST-GCN Block\n",
    "class STGCNBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, A, temporal_kernel_size=3, stride=1, residual=True):\n",
    "        super(STGCNBlock, self).__init__()\n",
    "        self.gcn = GraphConv(in_channels, out_channels, A)\n",
    "        self.tcn = nn.Conv2d(out_channels, out_channels, kernel_size=(temporal_kernel_size, 1), \n",
    "                             padding=(temporal_kernel_size//2, 0), stride=(stride, 1))\n",
    "        self.bn = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "        self.residual = residual\n",
    "        if residual:\n",
    "            self.res_conv = nn.Conv2d(in_channels, out_channels, kernel_size=(1, 1), stride=(stride, 1))\n",
    "            self.res_bn = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_gcn = self.gcn(x)\n",
    "        x_tcn = self.tcn(x_gcn)\n",
    "        x_tcn = self.bn(x_tcn)\n",
    "        if self.residual:\n",
    "            res = self.res_conv(x)\n",
    "            res = self.res_bn(res)\n",
    "            x_tcn = x_tcn + res\n",
    "        x_tcn = self.relu(x_tcn)\n",
    "        x_tcn = self.dropout(x_tcn)\n",
    "        return x_tcn\n",
    "\n",
    "# Enhanced ST-GCN Model\n",
    "class EnhancedSTGCN(nn.Module):\n",
    "    def __init__(self, in_channels=3, num_joints=18, num_classes=2, graph_edges=None):\n",
    "        super(EnhancedSTGCN, self).__init__()\n",
    "        self.num_joints = num_joints\n",
    "        self.A = create_adjacency_matrix(num_joints, graph_edges)\n",
    "        \n",
    "        self.block1 = STGCNBlock(in_channels, 64, self.A, temporal_kernel_size=3)\n",
    "        self.block2 = STGCNBlock(64, 64, self.A, temporal_kernel_size=3)\n",
    "        self.block3 = STGCNBlock(64, 128, self.A, temporal_kernel_size=3, stride=2)\n",
    "        self.block4 = STGCNBlock(128, 128, self.A, temporal_kernel_size=3)\n",
    "        self.block5 = STGCNBlock(128, 256, self.A, temporal_kernel_size=3, stride=2)\n",
    "        self.block6 = STGCNBlock(256, 256, self.A, temporal_kernel_size=3)\n",
    "        \n",
    "        self.pool = nn.AdaptiveAvgPool2d((1, num_joints))\n",
    "        self.fc = nn.Linear(256 * num_joints, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        x = self.block3(x)\n",
    "        x = self.block4(x)\n",
    "        x = self.block5(x)\n",
    "        x = self.block6(x)\n",
    "        x = self.pool(x)\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# Load the pre-trained model\n",
    "model_path = \"enhanced_stgcn_fall_detection.pth\"\n",
    "try:\n",
    "    model = EnhancedSTGCN(in_channels=3, num_joints=18, num_classes=2, graph_edges=edges).to(device)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.eval()\n",
    "    print(f\"Model loaded successfully from {model_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model: {e}\")\n",
    "    exit()\n",
    "\n",
    "# Initialize MediaPipe Pose\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose(static_image_mode=False, model_complexity=1, \n",
    "                    enable_segmentation=False, min_detection_confidence=0.7)\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Define the 18 landmarks in the same order as used during training\n",
    "landmark_indices = {\n",
    "    \"Nose\": 0, \"Right Shoulder\": 12, \"Right Elbow\": 14, \"Right Wrist\": 16,\n",
    "    \"Left Shoulder\": 11, \"Left Elbow\": 13, \"Left Wrist\": 15, \"Right Hip\": 24,\n",
    "    \"Right Knee\": 26, \"Right Ankle\": 28, \"Left Hip\": 23, \"Left Knee\": 25,\n",
    "    \"Left Ankle\": 27, \"Right Eye\": 5, \"Left Eye\": 2, \"Right Ear\": 8, \"Left Ear\": 7\n",
    "}\n",
    "\n",
    "# Initialize buffer and settings\n",
    "buffer = []\n",
    "prob_buffer = []\n",
    "window_size = 30\n",
    "step_size = 1\n",
    "threshold = 0.5\n",
    "frame_count = 0\n",
    "fps = 30\n",
    "\n",
    "# Graph settings\n",
    "graph_height = 100\n",
    "graph_width = 400\n",
    "max_points = 100\n",
    "\n",
    "# Start webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Unable to open camera.\")\n",
    "    exit()\n",
    "\n",
    "_, frame = cap.read()\n",
    "if frame is not None:\n",
    "    frame_height, frame_width = frame.shape[:2]\n",
    "else:\n",
    "    frame_height, frame_width = 480, 640\n",
    "graph_y_offset = frame_height - graph_height - 10\n",
    "\n",
    "# Main loop\n",
    "while True:\n",
    "    success, image = cap.read()\n",
    "    if not success:\n",
    "        print(\"Error: Unable to read frame from camera.\")\n",
    "        break\n",
    "    \n",
    "    frame_count += 1\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    results = pose.process(image_rgb)\n",
    "    \n",
    "    skeleton_frame = np.zeros((18, 3))\n",
    "    \n",
    "    if results.pose_landmarks:\n",
    "        landmarks = results.pose_landmarks.landmark\n",
    "        left_hip = landmarks[23]\n",
    "        right_hip = landmarks[24]\n",
    "        ref_x = (left_hip.x + right_hip.x) / 2\n",
    "        ref_y = (left_hip.y + right_hip.y) / 2\n",
    "        ref_z = (left_hip.z + right_hip.z) / 2\n",
    "        left_shoulder = landmarks[11]\n",
    "        right_shoulder = landmarks[12]\n",
    "        neck_x = (left_shoulder.x + right_shoulder.x) / 2\n",
    "        neck_y = (left_shoulder.y + right_shoulder.y) / 2\n",
    "        neck_z = (left_shoulder.z + right_shoulder.z) / 2\n",
    "        \n",
    "        for i, part in enumerate(landmark_order):\n",
    "            if part == \"Neck\":\n",
    "                x = neck_x - ref_x\n",
    "                y = neck_y - ref_y\n",
    "                z = neck_z - ref_z\n",
    "            else:\n",
    "                lm = landmarks[landmark_indices[part]]\n",
    "                x = lm.x - ref_x\n",
    "                y = lm.y - ref_y\n",
    "                z = lm.z - ref_z\n",
    "            skeleton_frame[i] = [x, y, z]\n",
    "        \n",
    "        mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS)\n",
    "    \n",
    "    buffer.append(skeleton_frame)\n",
    "    \n",
    "    if len(buffer) >= window_size and frame_count % step_size == 0:\n",
    "        window = buffer[-window_size:]\n",
    "        skeleton_sequence = np.stack(window, axis=0)\n",
    "        skeleton_sequence = torch.tensor(skeleton_sequence, dtype=torch.float32).permute(2, 0, 1).unsqueeze(0)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = model(skeleton_sequence.to(device))\n",
    "            probabilities = torch.softmax(output, dim=1)\n",
    "            fall_prob = probabilities[0, 1].item()\n",
    "            prediction = 1 if fall_prob >= threshold else 0\n",
    "        \n",
    "        prob_buffer.append(fall_prob * 100)\n",
    "        if len(prob_buffer) > max_points:\n",
    "            prob_buffer.pop(0)\n",
    "        \n",
    "        text = f\"Fall Detected ({fall_prob:.2f})\" if prediction == 1 else f\"No Fall ({fall_prob:.2f})\"\n",
    "        color = (0, 0, 255) if prediction == 1 else (0, 255, 0)\n",
    "        cv2.putText(image, text, (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, color, 2)\n",
    "        \n",
    "        info_text = f\"Threshold: {threshold:.2f} (+/-), Window: {window_size}, Step: {step_size}\"\n",
    "        cv2.putText(image, info_text, (50, 100), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "    elif len(buffer) < window_size:\n",
    "        cv2.putText(image, f\"Buffering... ({len(buffer)}/{window_size})\", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "    \n",
    "    graph_img = np.zeros((graph_height, graph_width, 3), dtype=np.uint8)\n",
    "    if prob_buffer:\n",
    "        cv2.line(graph_img, (0, graph_height - 10), (graph_width, graph_height - 10), (255, 255, 255), 1)\n",
    "        cv2.line(graph_img, (10, 0), (10, graph_height), (255, 255, 255), 1)\n",
    "        cv2.putText(graph_img, \"Time (s)\", (graph_width - 50, graph_height - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255, 255, 255), 1)\n",
    "        cv2.putText(graph_img, \"Prob (%)\", (5, 15), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255, 255, 255), 1)\n",
    "        cv2.putText(graph_img, \"100\", (2, 15), cv2.FONT_HERSHEY_SIMPLEX, 0.3, (255, 255, 255), 1)\n",
    "        cv2.putText(graph_img, \"0\", (2, graph_height - 15), cv2.FONT_HERSHEY_SIMPLEX, 0.3, (255, 255, 255), 1)\n",
    "        \n",
    "        points = []\n",
    "        for i, prob in enumerate(prob_buffer):\n",
    "            x = int(i * (graph_width - 20) / max_points) + 10\n",
    "            y = int((1 - prob / 100) * (graph_height - 20)) + 10\n",
    "            points.append((x, y))\n",
    "        \n",
    "        for i in range(1, len(points)):\n",
    "            cv2.line(graph_img, points[i-1], points[i], (0, 255, 0), 1)\n",
    "        \n",
    "        time_span = len(prob_buffer) / fps\n",
    "        cv2.putText(graph_img, f\"{time_span:.1f}s\", (graph_width - 30, graph_height - 15), cv2.FONT_HERSHEY_SIMPLEX, 0.3, (255, 255, 255), 1)\n",
    "    \n",
    "    image[graph_y_offset:graph_y_offset + graph_height, 10:10 + graph_width] = graph_img\n",
    "    \n",
    "    cv2.imshow(\"Live Fall Detection\", image)\n",
    "    \n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    if key == ord('q'):\n",
    "        break\n",
    "    elif key == ord('+') or key == ord('='):\n",
    "        threshold = min(1.0, threshold + 0.05)\n",
    "    elif key == ord('-'):\n",
    "        threshold = max(0.0, threshold - 0.05)\n",
    "    elif key == ord('w'):\n",
    "        window_size = min(60, window_size + 5)\n",
    "    elif key == ord('s'):\n",
    "        window_size = max(10, window_size - 5)\n",
    "    elif key == ord('d'):\n",
    "        step_size = min(10, step_size + 1)\n",
    "    elif key == ord('a'):\n",
    "        step_size = max(1, step_size - 1)\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "pose.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fart",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
