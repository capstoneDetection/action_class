{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cb8aba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading model: [Errno 2] No such file or directory: '../AI_Train/Models/dnn_fall_detection.pth'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1744339986.210187 2559143 gl_context.cc:369] GL version: 2.1 (2.1 Metal - 88.1), renderer: Apple M1\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "W0000 00:00:1744339986.334090 2559983 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1744339986.385033 2559987 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "2025-04-11 11:53:06.569 python[27224:2559143] WARNING: AVCaptureDeviceTypeExternal is deprecated for Continuity Cameras. Please use AVCaptureDeviceTypeContinuityCamera and add NSCameraUseContinuityCameraDeviceType to your Info.plist.\n",
      "W0000 00:00:1744340006.138171 2559986 landmark_projection_calculator.cc:186] Using NORM_RECT without IMAGE_DIMENSIONS is only supported for the square ROI. Provide IMAGE_DIMENSIONS or use PROJECTION_MATRIX.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m현재 셀 또는 이전 셀에서 코드를 실행하는 동안 Kernel이 충돌했습니다. \n",
      "\u001b[1;31m셀의 코드를 검토하여 가능한 오류 원인을 식별하세요. \n",
      "\u001b[1;31m자세한 내용을 보려면 <a href='https://aka.ms/vscodeJupyterKernelCrash'>여기</a>를 클릭하세요. \n",
      "\u001b[1;31m자세한 내용은 Jupyter <a href='command:jupyter.viewOutput'>로그</a>를 참조하세요."
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# DNN Model Definition\n",
    "class FallDetectionDNN(nn.Module):\n",
    "    def __init__(self, input_size=1620, hidden_size=512, num_classes=2):\n",
    "        super(FallDetectionDNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size // 2)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc3 = nn.Linear(hidden_size // 2, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Load the trained model\n",
    "model_path = \"../AI_Train/Models/dnn_fall_detection.pth\"\n",
    "try:\n",
    "    model = FallDetectionDNN(input_size=1620).to(device)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.eval()\n",
    "    print(f\"Model loaded successfully from {model_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model: {e}\")\n",
    "    exit()\n",
    "\n",
    "# Initialize MediaPipe Pose\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose(static_image_mode=False, model_complexity=1, \n",
    "                    enable_segmentation=False, min_detection_confidence=0.7)\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Landmark order (same as training)\n",
    "landmark_order = [\n",
    "    \"Nose\", \"Neck\", \"Right Shoulder\", \"Right Elbow\", \"Right Wrist\",\n",
    "    \"Left Shoulder\", \"Left Elbow\", \"Left Wrist\", \"Right Hip\", \"Right Knee\",\n",
    "    \"Right Ankle\", \"Left Hip\", \"Left Knee\", \"Left Ankle\", \"Right Eye\",\n",
    "    \"Left Eye\", \"Right Ear\", \"Left Ear\"\n",
    "]\n",
    "\n",
    "landmark_indices = {\n",
    "    \"Nose\": 0, \"Right Shoulder\": 12, \"Right Elbow\": 14, \"Right Wrist\": 16,\n",
    "    \"Left Shoulder\": 11, \"Left Elbow\": 13, \"Left Wrist\": 15, \"Right Hip\": 24,\n",
    "    \"Right Knee\": 26, \"Right Ankle\": 28, \"Left Hip\": 23, \"Left Knee\": 25,\n",
    "    \"Left Ankle\": 27, \"Right Eye\": 5, \"Left Eye\": 2, \"Right Ear\": 8, \"Left Ear\": 7\n",
    "}\n",
    "\n",
    "# Buffers and settings\n",
    "buffer = []\n",
    "prob_buffer = []\n",
    "window_size = 30  # Matches training (1 second at 30 FPS)\n",
    "step_size = 2     # Frequent checks\n",
    "threshold = 0.5   # Adjusted based on previous feedback\n",
    "frame_count = 0\n",
    "fps = 30  # Assumed FPS\n",
    "\n",
    "# Graph settings\n",
    "graph_height = 100\n",
    "graph_width = 400\n",
    "max_points = 100\n",
    "\n",
    "# Start webcam\n",
    "cap = cv2.VideoCapture(0)\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Unable to open camera.\")\n",
    "    exit()\n",
    "\n",
    "# Frame dimensions\n",
    "_, frame = cap.read()\n",
    "if frame is not None:\n",
    "    frame_height, frame_width = frame.shape[:2]\n",
    "else:\n",
    "    frame_height, frame_width = 480, 640\n",
    "graph_y_offset = frame_height - graph_height - 10\n",
    "\n",
    "# Main loop\n",
    "while True:\n",
    "    success, image = cap.read()\n",
    "    if not success:\n",
    "        print(\"Error: Unable to read frame from camera.\")\n",
    "        break\n",
    "    \n",
    "    frame_count += 1\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    results = pose.process(image_rgb)\n",
    "    \n",
    "    skeleton_frame = np.zeros((18, 3))\n",
    "    \n",
    "    if results.pose_landmarks:\n",
    "        landmarks = results.pose_landmarks.landmark\n",
    "        left_hip = landmarks[23]\n",
    "        right_hip = landmarks[24]\n",
    "        ref_x = (left_hip.x + right_hip.x) / 2\n",
    "        ref_y = (left_hip.y + right_hip.y) / 2\n",
    "        ref_z = (left_hip.z + right_hip.z) / 2\n",
    "        left_shoulder = landmarks[11]\n",
    "        right_shoulder = landmarks[12]\n",
    "        neck_x = (left_shoulder.x + right_shoulder.x) / 2\n",
    "        neck_y = (left_shoulder.y + right_shoulder.y) / 2\n",
    "        neck_z = (left_shoulder.z + right_shoulder.z) / 2\n",
    "        \n",
    "        for i, part in enumerate(landmark_order):\n",
    "            if part == \"Neck\":\n",
    "                x = neck_x - ref_x\n",
    "                y = neck_y - ref_y\n",
    "                z = neck_z - ref_z\n",
    "            else:\n",
    "                lm = landmarks[landmark_indices[part]]\n",
    "                x = lm.x - ref_x\n",
    "                y = lm.y - ref_y\n",
    "                z = lm.z - ref_z\n",
    "            skeleton_frame[i] = [x, y, z]\n",
    "        \n",
    "        mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS)\n",
    "    \n",
    "    buffer.append(skeleton_frame)\n",
    "    \n",
    "    if len(buffer) >= window_size and frame_count % step_size == 0:\n",
    "        window = buffer[-window_size:]  # Most recent 30 frames\n",
    "        skeleton_sequence = np.stack(window, axis=0)  # (30, 18, 3)\n",
    "        skeleton_sequence = skeleton_sequence.flatten()  # (1620,)\n",
    "        skeleton_tensor = torch.tensor(skeleton_sequence, dtype=torch.float32).unsqueeze(0).to(device)  # (1, 1620)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output = model(skeleton_tensor)\n",
    "            probabilities = torch.softmax(output, dim=1)\n",
    "            fall_prob = probabilities[0, 1].item()\n",
    "            prediction = 1 if fall_prob >= threshold else 0\n",
    "        \n",
    "        prob_buffer.append(fall_prob * 100)\n",
    "        if len(prob_buffer) > max_points:\n",
    "            prob_buffer.pop(0)\n",
    "        \n",
    "        text = f\"Fall Detected ({fall_prob:.2f})\" if prediction == 1 else f\"No Fall ({fall_prob:.2f})\"\n",
    "        color = (0, 0, 255) if prediction == 1 else (0, 255, 0)\n",
    "        cv2.putText(image, text, (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, color, 2)\n",
    "        \n",
    "        info_text = f\"Threshold: {threshold:.2f} (+/-), Window: {window_size}, Step: {step_size}\"\n",
    "        cv2.putText(image, info_text, (50, 100), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "    elif len(buffer) < window_size:\n",
    "        cv2.putText(image, f\"Buffering... ({len(buffer)}/{window_size})\", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "    \n",
    "    # Draw graph\n",
    "    graph_img = np.zeros((graph_height, graph_width, 3), dtype=np.uint8)\n",
    "    if prob_buffer:\n",
    "        cv2.line(graph_img, (0, graph_height - 10), (graph_width, graph_height - 10), (255, 255, 255), 1)\n",
    "        cv2.line(graph_img, (10, 0), (10, graph_height), (255, 255, 255), 1)\n",
    "        cv2.putText(graph_img, \"Time (s)\", (graph_width - 50, graph_height - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255, 255, 255), 1)\n",
    "        cv2.putText(graph_img, \"Prob (%)\", (5, 15), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255, 255, 255), 1)\n",
    "        cv2.putText(graph_img, \"100\", (2, 15), cv2.FONT_HERSHEY_SIMPLEX, 0.3, (255, 255, 255), 1)\n",
    "        cv2.putText(graph_img, \"0\", (2, graph_height - 15), cv2.FONT_HERSHEY_SIMPLEX, 0.3, (255, 255, 255), 1)\n",
    "        \n",
    "        points = []\n",
    "        for i, prob in enumerate(prob_buffer):\n",
    "            x = int(i * (graph_width - 20) / max_points) + 10\n",
    "            y = int((1 - prob / 100) * (graph_height - 20)) + 10\n",
    "            points.append((x, y))\n",
    "        \n",
    "        for i in range(1, len(points)):\n",
    "            cv2.line(graph_img, points[i-1], points[i], (0, 255, 0), 1)\n",
    "        \n",
    "        time_span = len(prob_buffer) / fps\n",
    "        cv2.putText(graph_img, f\"{time_span:.1f}s\", (graph_width - 30, graph_height - 15), cv2.FONT_HERSHEY_SIMPLEX, 0.3, (255, 255, 255), 1)\n",
    "    \n",
    "    image[graph_y_offset:graph_y_offset + graph_height, 10:10 + graph_width] = graph_img\n",
    "    \n",
    "    cv2.imshow(\"Live Fall Detection (DNN)\", image)\n",
    "    \n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    if key == ord('q'):\n",
    "        break\n",
    "    elif key == ord('+') or key == ord('='):\n",
    "        threshold = min(1.0, threshold + 0.05)\n",
    "    elif key == ord('-'):\n",
    "        threshold = max(0.0, threshold - 0.05)\n",
    "    elif key == ord('w'):\n",
    "        window_size = min(60, window_size + 5)\n",
    "    elif key == ord('s'):\n",
    "        window_size = max(10, window_size - 5)\n",
    "    elif key == ord('d'):\n",
    "        step_size = min(10, step_size + 1)\n",
    "    elif key == ord('a'):\n",
    "        step_size = max(1, step_size - 1)\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "pose.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fart",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
