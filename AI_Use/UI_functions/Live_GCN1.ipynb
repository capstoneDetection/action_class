{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from fall_alert_api import get_id_by_device, send_status_update #\n",
    "import time #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device (use GPU if available, otherwise CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the ST-GCN model class (unchanged)\n",
    "class STGCN(nn.Module):\n",
    "    def __init__(self, in_channels=3, num_joints=18, num_classes=2):\n",
    "        super(STGCN, self).__init__()\n",
    "        self.num_joints = num_joints\n",
    "        self.conv1 = nn.Conv2d(in_channels, 64, kernel_size=(1, 1))\n",
    "        self.conv2 = nn.Conv2d(64, 64, kernel_size=(3, 1), padding=(1, 0))\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=(1, 1))\n",
    "        self.conv4 = nn.Conv2d(128, 128, kernel_size=(3, 1), padding=(1, 0))\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.bn4 = nn.BatchNorm2d(128)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.pool = nn.AdaptiveAvgPool2d((1, num_joints))\n",
    "        self.fc = nn.Linear(128 * num_joints, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.relu(self.bn3(self.conv3(x)))\n",
    "        x = self.relu(self.bn4(self.conv4(x)))\n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained model (unchanged)\n",
    "model_path = \"../../AI_Train/Models/stgcn_fall_detection.pth\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully from ../../AI_Train/Models/stgcn_fall_detection.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\am332\\AppData\\Local\\Temp\\ipykernel_9628\\2249852570.py:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_path, map_location=device))\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    model = STGCN(in_channels=3, num_joints=18, num_classes=2).to(device)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.eval()\n",
    "    print(f\"Model loaded successfully from {model_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model: {e}\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize MediaPipe Pose (unchanged)\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose(static_image_mode=False, model_complexity=1, \n",
    "                    enable_segmentation=False, min_detection_confidence=0.5)\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Define the 18 landmarks in the same order as used during training (unchanged)\n",
    "landmark_order = [\n",
    "    \"Nose\", \"Neck\", \"Right Shoulder\", \"Right Elbow\", \"Right Wrist\",\n",
    "    \"Left Shoulder\", \"Left Elbow\", \"Left Wrist\", \"Right Hip\", \"Right Knee\",\n",
    "    \"Right Ankle\", \"Left Hip\", \"Left Knee\", \"Left Ankle\", \"Right Eye\",\n",
    "    \"Left Eye\", \"Right Ear\", \"Left Ear\"\n",
    "]\n",
    "\n",
    "landmark_indices = {\n",
    "    \"Nose\": 0, \"Right Shoulder\": 12, \"Right Elbow\": 14, \"Right Wrist\": 16,\n",
    "    \"Left Shoulder\": 11, \"Left Elbow\": 13, \"Left Wrist\": 15, \"Right Hip\": 24,\n",
    "    \"Right Knee\": 26, \"Right Ankle\": 28, \"Left Hip\": 23, \"Left Knee\": 25,\n",
    "    \"Left Ankle\": 27, \"Right Eye\": 5, \"Left Eye\": 2, \"Right Ear\": 8, \"Left Ear\": 7\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize buffer to store skeleton data and probabilities for graph\n",
    "buffer = []\n",
    "prob_buffer = []  # Store fall probabilities for plotting\n",
    "window_size = 30  # Number of frames per window\n",
    "step_size = 1     # Step size for sliding window\n",
    "threshold = 0.8   # Initial threshold for fall detection\n",
    "frame_count = 0   # Track frames for time calculation\n",
    "fps = 30          # Assumed FPS for time axis (adjust if known)\n",
    "\n",
    "# Graph settings\n",
    "graph_height = 100  # Height of graph area in pixels\n",
    "graph_width = 400   # Width of graph area in pixels\n",
    "max_points = 100    # Number of probability points to display (adjustable)\n",
    "camera_id = 1\n",
    "\n",
    "last_fall_alert_time = 0  #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start capturing video from the default camera\n",
    "cap = cv2.VideoCapture(camera_id)\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Unable to open camera.\")\n",
    "    exit()\n",
    "\n",
    "# Get frame dimensions (to position graph at bottom)\n",
    "_, frame = cap.read()\n",
    "if frame is not None:\n",
    "    frame_height, frame_width = frame.shape[:2]\n",
    "else:\n",
    "    frame_height, frame_width = 480, 640  # Fallback dimensions\n",
    "graph_y_offset = frame_height - graph_height - 10  # 10-pixel margin from bottom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success: {'message': '상태 변경 성공'}\n",
      "Fall alert sent successfully.\n",
      "Success: {'message': '상태 변경 성공'}\n",
      "Fall alert sent successfully.\n",
      "Success: {'message': '상태 변경 성공'}\n",
      "Fall alert sent successfully.\n",
      "Success: {'message': '상태 변경 성공'}\n",
      "Fall alert sent successfully.\n"
     ]
    }
   ],
   "source": [
    "# Main loop for real-time processing\n",
    "while True:\n",
    "    success, image = cap.read()\n",
    "    if not success:\n",
    "        print(\"Error: Unable to read frame from camera.\")\n",
    "        break\n",
    "    \n",
    "    frame_count += 1  # Increment frame counter\n",
    "    # Convert the frame to RGB for MediaPipe processing\n",
    "    image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    results = pose.process(image_rgb)\n",
    "    \n",
    "    # Initialize skeleton frame with zeros (18 joints, 3 coordinates: x, y, z)\n",
    "    skeleton_frame = np.zeros((18, 3))\n",
    "    \n",
    "    if results.pose_landmarks:\n",
    "        landmarks = results.pose_landmarks.landmark\n",
    "        \n",
    "        # Compute reference point: average of left and right hip\n",
    "        left_hip = landmarks[23]\n",
    "        right_hip = landmarks[24]\n",
    "        ref_x = (left_hip.x + right_hip.x) / 2\n",
    "        ref_y = (left_hip.y + right_hip.y) / 2\n",
    "        ref_z = (left_hip.z + right_hip.z) / 2\n",
    "        \n",
    "        # Compute neck as the average of left and right shoulders\n",
    "        left_shoulder = landmarks[11]\n",
    "        right_shoulder = landmarks[12]\n",
    "        neck_x = (left_shoulder.x + right_shoulder.x) / 2\n",
    "        neck_y = (left_shoulder.y + right_shoulder.y) / 2\n",
    "        neck_z = (left_shoulder.z + right_shoulder.z) / 2\n",
    "        \n",
    "        # Extract relative coordinates for the 18 landmarks\n",
    "        for i, part in enumerate(landmark_order):\n",
    "            if part == \"Neck\":\n",
    "                x = neck_x - ref_x\n",
    "                y = neck_y - ref_y\n",
    "                z = neck_z - ref_z\n",
    "            else:\n",
    "                lm = landmarks[landmark_indices[part]]\n",
    "                x = lm.x - ref_x\n",
    "                y = lm.y - ref_y\n",
    "                z = lm.z - ref_z\n",
    "            skeleton_frame[i] = [x, y, z]\n",
    "        \n",
    "        # Draw the detected landmarks on the image for visualization\n",
    "        mp_drawing.draw_landmarks(image, results.pose_landmarks, mp_pose.POSE_CONNECTIONS)\n",
    "    \n",
    "    # Add the current skeleton frame to the buffer\n",
    "    buffer.append(skeleton_frame)\n",
    "    \n",
    "    # Process sliding windows if enough frames are available\n",
    "    if len(buffer) >= window_size:\n",
    "        # Process the most recent window\n",
    "        start_idx = max(0, len(buffer) - window_size)\n",
    "        window = buffer[start_idx:start_idx + window_size]\n",
    "        \n",
    "        # Convert window to tensor with shape (1, 3, window_size, 18)\n",
    "        skeleton_sequence = np.stack(window, axis=0)  # Shape: (window_size, 18, 3)\n",
    "        skeleton_sequence = torch.tensor(skeleton_sequence, dtype=torch.float32).permute(2, 0, 1).unsqueeze(0)  # Shape: (1, 3, window_size, 18)\n",
    "        \n",
    "        # Perform inference\n",
    "        with torch.no_grad():\n",
    "            output = model(skeleton_sequence.to(device))\n",
    "            probabilities = torch.softmax(output, dim=1)\n",
    "            fall_prob = probabilities[0, 1].item()\n",
    "            prediction = 1 if fall_prob >= threshold else 0\n",
    "            \n",
    "            #\n",
    "            if prediction == 1:\n",
    "                current_time = time.time()\n",
    "                should_send_alert = (current_time - last_fall_alert_time >= 60) or (last_fall_alert_time == 0)\n",
    "                \n",
    "                if should_send_alert:\n",
    "                    user_id = get_id_by_device(device_id=1)\n",
    "                    success = user_id is not None and send_status_update(user_id, status=1)\n",
    "\n",
    "                    if success:\n",
    "                        print(\"Fall alert sent successfully.\")\n",
    "                        last_fall_alert_time = current_time\n",
    "                    else:\n",
    "                        print(\"Failed to send fall alert.\")\n",
    "            else:\n",
    "                last_fall_alert_time = 0  # 낙상이 끝난 경우 시간 초기화\n",
    "            #\n",
    "        # Store probability for graph\n",
    "        prob_buffer.append(fall_prob * 100)  # Convert to percentage\n",
    "        if len(prob_buffer) > max_points:\n",
    "            prob_buffer.pop(0)  # Keep only the latest max_points\n",
    "        \n",
    "        # Display the prediction and probability on the frame\n",
    "        if prediction == 1:\n",
    "            text = f\"Fall Detected! ({fall_prob:.2f})\"\n",
    "            color = (0, 0, 255)  # Red\n",
    "        else:\n",
    "            text = f\"Fall Detected ({fall_prob:.2f})\"  # Changed from \"No Fall\"\n",
    "            color = (0, 255, 0)  # Green\n",
    "        cv2.putText(image, text, (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, color, 2)\n",
    "        \n",
    "        # Display the current threshold and window info\n",
    "        info_text = f\"Threshold: {threshold:.2f} (+/-), Window: {window_size}, Step: {step_size}\"\n",
    "        cv2.putText(image, info_text, (50, 100), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "    else:\n",
    "        # Display buffering message until enough frames are collected\n",
    "        cv2.putText(image, f\"Buffering... ({len(buffer)}/{window_size})\", (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "    \n",
    "    # Draw live graph at the bottom\n",
    "    graph_img = np.zeros((graph_height, graph_width, 3), dtype=np.uint8)  # Black background\n",
    "    if prob_buffer:\n",
    "        # Draw axes\n",
    "        cv2.line(graph_img, (0, graph_height - 10), (graph_width, graph_height - 10), (255, 255, 255), 1)  # X-axis\n",
    "        cv2.line(graph_img, (10, 0), (10, graph_height), (255, 255, 255), 1)  # Y-axis\n",
    "        \n",
    "        # Draw axis labels\n",
    "        cv2.putText(graph_img, \"Time (s)\", (graph_width - 50, graph_height - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255, 255, 255), 1)\n",
    "        cv2.putText(graph_img, \"Prob (%)\", (5, 15), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255, 255, 255), 1)\n",
    "        cv2.putText(graph_img, \"100\", (2, 15), cv2.FONT_HERSHEY_SIMPLEX, 0.3, (255, 255, 255), 1)\n",
    "        cv2.putText(graph_img, \"0\", (2, graph_height - 15), cv2.FONT_HERSHEY_SIMPLEX, 0.3, (255, 255, 255), 1)\n",
    "        \n",
    "        # Plot probabilities\n",
    "        points = []\n",
    "        for i, prob in enumerate(prob_buffer):\n",
    "            x = int(i * (graph_width - 20) / max_points) + 10  # Scale x to graph width\n",
    "            y = int((1 - prob / 100) * (graph_height - 20)) + 10  # Scale y (0 at bottom, 100 at top)\n",
    "            points.append((x, y))\n",
    "        \n",
    "        # Draw lines between points\n",
    "        for i in range(1, len(points)):\n",
    "            cv2.line(graph_img, points[i-1], points[i], (0, 255, 0), 1)\n",
    "        \n",
    "        # Draw time label for x-axis (approximate seconds)\n",
    "        time_span = len(prob_buffer) / fps\n",
    "        cv2.putText(graph_img, f\"{time_span:.1f}s\", (graph_width - 30, graph_height - 15), cv2.FONT_HERSHEY_SIMPLEX, 0.3, (255, 255, 255), 1)\n",
    "    \n",
    "    # Place graph at bottom of frame\n",
    "    image[graph_y_offset:graph_y_offset + graph_height, 10:10 + graph_width] = graph_img\n",
    "    \n",
    "    # Show the processed frame\n",
    "    cv2.imshow(\"Live Fall Detection\", image)\n",
    "    \n",
    "    # Handle key presses to adjust threshold, window size, step size, or exit\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    if key == ord('q'):  # Exit on 'q'\n",
    "        break\n",
    "    elif key == ord('+') or key == ord('='):  # Increase threshold\n",
    "        threshold = min(1.0, threshold + 0.05)\n",
    "    elif key == ord('-'):  # Decrease threshold\n",
    "        threshold = max(0.0, threshold - 0.05)\n",
    "    elif key == ord('w'):  # Increase window size\n",
    "        window_size = min(60, window_size + 5)\n",
    "    elif key == ord('s'):  # Decrease window size\n",
    "        window_size = max(10, window_size - 5)\n",
    "    elif key == ord('d'):  # Increase step size\n",
    "        step_size = min(10, step_size + 1)\n",
    "    elif key == ord('a'):  # Decrease step size\n",
    "        step_size = max(1, step_size - 1)\n",
    "\n",
    "# Release resources\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "pose.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "grad",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
